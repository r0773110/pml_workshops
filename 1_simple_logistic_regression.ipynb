{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://githubtocolab.com/PML-UCF/pml_workshops/blob/main/1_simple_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Logistic Regression\n",
    "\n",
    "Loading dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = pd.read_csv('https://raw.githubusercontent.com/PML-UCF/pml_workshops/main/data/social_network_ads.csv')\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = dataset.iloc[:, [2,3]].values\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "Y = dataset.iloc[:, 4].values[:,np.newaxis]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary Logistic Regression\n",
    "\n",
    "Lets consider:\n",
    "\n",
    "$\\hat{y}=p(y=1 \\mid x)$\n",
    "\n",
    "$\\hat{y}$ is the probability that $y=1$, given $x$\n",
    "\n",
    "$1-\\hat{y}=p(y=0 \\mid x)$\n",
    "\n",
    "$\\hat{y}=f(u)$, $u=x w^{T}+b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Loss function\n",
    "$\\operatorname{cost}\\left(\\hat{y}, y\\right)= \\begin{cases}-\\log \\left(\\hat{y}\\right) & \\text { if } y=1 \\\\ -\\log \\left(1-\\hat{y}\\right) \\text { if } y=0\\end{cases}$\n",
    "\n",
    "\n",
    "## Simplified Loss Function\n",
    "$\\operatorname{Cost}\\left(\\hat{y}, y\\right)=-y \\log \\left(\\hat{y}\\right)-(1-y) \\log \\left(1-\\hat{y}\\right)$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Deriving Gradient\n",
    "\n",
    "$z=w_{1} x_{1}+w_{2} x_{2}+b$\n",
    "\n",
    "$\\hat{y}=a=\\sigma(z)$\n",
    "\n",
    "$Loss \\rightarrow L(\\hat{y}, y)$\n",
    "\n",
    "**For $w_{1}$**:\n",
    "\n",
    "$\\frac{\\partial(L)}{\\partial w_{1}}=\\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial(z)}{\\partial w_{1}}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a}=\\frac{\\partial}{\\partial a}(-y \\log a-(1-y) \\log (1-a))$\n",
    "\n",
    "$=-y\\left(\\frac{1}{a}\\right)-(-1) \\frac{(1-y)}{(1-a)}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a}=\\left(\\frac{-y}{a}\\right)+\\left(\\frac{1-y}{1-a}\\right)$\n",
    "\n",
    "$\\frac{\\partial a}{\\partial z}=a(1-a)$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w_{1}}=x_{1}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\\frac{\\partial(L)}{\\partial w_{1}}=(a-y) \\cdot x_{1}$,\n",
    "\n",
    "and\n",
    "\n",
    "$\\frac{\\partial(L)}{\\partial b}=(a-y)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(a):\n",
    "    return a*(1-a)\n",
    "\n",
    "def loss(weights, bias, x, y):\n",
    "    a = forward(weights, bias, x)\n",
    "    return (-1/x.shape[0])*(np.sum((y*np.log(a)) + ((1-y)*(np.log(1-a)))))\n",
    "\n",
    "def d_loss(a, y):\n",
    "    return ((1-y)/(1-a)) - y/a\n",
    "\n",
    "def forward(weights, bias, x):\n",
    "    z = x.dot(weights.T) + bias\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def backward(a, y):\n",
    "    # gradient = d_loss(a, y)*d_sigmoid(a)\n",
    "    gradient = (a - y)\n",
    "    return gradient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X1, X2 = np.meshgrid(np.arange(start = X[:, 0].min() - 1, stop = X[:, 0].max() + 1, step = 0.1),\n",
    "                    (np.arange(start = X[:, 1].min() -1, stop = X[:, 1].max() + 1, step = 0.1)))\n",
    "def plot(loss, pred, fig, ax1, ax2):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    \n",
    "    ax1.grid(True)\n",
    "    ax1.plot(loss)\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_xlim(0, epochs)\n",
    "\n",
    "    ax2.contourf(X1, X2, pred,\n",
    "                alpha = 0.50, cmap = ListedColormap(('red', 'green')))\n",
    "    ax2.set_xlim(X1.min(), X1.max())\n",
    "    ax2.set_ylim(X2.min(), X2.max())\n",
    "    for i, j in enumerate(np.unique(Y)):\n",
    "        ax2.scatter(X[Y[:,0] == j, 0], X[Y[:,0] == j, 1],\n",
    "                    color = ListedColormap(('red', 'green'))(i), label = j)\n",
    "\n",
    "    ax2.set_xlabel('Age')\n",
    "    ax2.set_ylabel('Estimated Salary')\n",
    "    ax2.legend()\n",
    "    \n",
    "    display.update_display(fig,display_id=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop\n",
    "\n",
    "**Update model parameters in batches:**\n",
    "\n",
    "$w = w - \\alpha \\cdot \\frac{\\partial(L)}{\\partial w}$\n",
    "\n",
    "and\n",
    "\n",
    "$b = b - \\alpha \\cdot \\frac{\\partial(L)}{\\partial b}$\n",
    "\n",
    "$\\alpha \\rightarrow $ Learning rate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights = np.random.randn(1, X.shape[1])\n",
    "bias = np.zeros((1, 1))\n",
    "\n",
    "history = {'loss':[]}\n",
    "\n",
    "lr=0.01\n",
    "batch_size=4\n",
    "epochs=20\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
    "display.display(fig,display_id=1)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    shuffled_indices = np.random.permutation(X.shape[0])\n",
    "    x_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = Y[shuffled_indices]\n",
    "\n",
    "    # SGD with mini batches\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        xi = x_shuffled[i:i+batch_size]\n",
    "        yi = y_shuffled[i:i+batch_size]\n",
    "\n",
    "        a = forward(weights, bias, xi)\n",
    "        gradient = backward(a, yi)\n",
    "        \n",
    "        weights = weights - lr * (gradient.T @ xi)/batch_size\n",
    "        bias = bias - lr * gradient.mean()\n",
    "\n",
    "    history['loss'] += [loss(weights, bias, x_shuffled, y_shuffled)]\n",
    "\n",
    "    pred = forward(weights, bias, np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
    "    plot(history['loss'], pred, fig, ax1, ax2)\n",
    "    time.sleep(1)\n",
    "display.clear_output(wait=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " _______ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Tensorflow Automatic Differentiation and Gradients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tf_sigmoid(z):\n",
    "    return 1/(1+tf.exp(-z))\n",
    "\n",
    "def tf_forward(w, b, x):\n",
    "    z = x @ w + b\n",
    "    a = tf_sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def tf_loss(a, y):\n",
    "    return (-1/a.shape[0])*(tf.reduce_sum((y*tf.math.log(a)) + ((1-y)*(tf.math.log(1-a)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w = tf.Variable(tf.zeros((X.shape[1], 1)), name='w')\n",
    "b = tf.Variable(tf.random.uniform((1, 1)), name='b')\n",
    "\n",
    "history = {'loss':[]}\n",
    "lr=0.01\n",
    "batch_size=4\n",
    "epochs=20\n",
    "\n",
    "for ep in range(epochs):\n",
    "    shuffled_indices = np.random.permutation(X.shape[0])\n",
    "    x_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = Y[shuffled_indices]\n",
    "\n",
    "    # SGD with mini batches\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        xi = x_shuffled[i:i+batch_size]\n",
    "        yi = y_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Computing gradients with tf.GradientTape\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(w)\n",
    "            tape.watch(b)\n",
    "            a = tf_forward(w, b, xi)\n",
    "            l = tf_loss(a, yi)\n",
    "\n",
    "        [dl_dw, dl_db] = tape.gradient(l, [w, b])\n",
    "        \n",
    "        w = w - lr * dl_dw\n",
    "        b = b - lr * dl_db\n",
    "\n",
    "    history['loss'] += [tf_loss(tf_forward(w, b, x_shuffled), y_shuffled).numpy()]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = tf_forward(w,b,np.array([X1.ravel(), X2.ravel()]).T).numpy().reshape(X1.shape)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot(history['loss'], pred, fig, ax1, ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_________"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Tensorflow Model and Layers API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential(\n",
    "    Dense(1, activation='sigmoid')\n",
    ")\n",
    "\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy')\n",
    "history = model.fit(X,Y, batch_size=4, epochs=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot(history.history['loss'], pred, fig, ax1, ax2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " \n",
    " "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "interpreter": {
   "hash": "2c3bc7dc83c1a9b46d99db8b132f805da16834f865ffb3b3861549cbb2b21749"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}